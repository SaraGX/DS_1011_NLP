{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from gensim.models import FastText\n",
    "\n",
    "import os, re, csv, math, codecs\n",
    "import io\n",
    "\n",
    "from Encoder import CNN_Encoder, RNN_Encoder, RNN_Encoder_element_wise, CNN_Encoder_element_wise\n",
    "from train_test import test_model, train_model\n",
    "from NLI_DataLoader import NLI_Dataset, NLI_collate_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "data_dir = './hw2_data/'\n",
    "\n",
    "MAX_SENTENCE1_LENGTH = 32\n",
    "MAX_SENTENCE2_LENGTH = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5 genres in MNLI: ['telephone', 'fiction', 'slate', 'government', 'travel']\n"
     ]
    }
   ],
   "source": [
    "mnli_train = pd.read_csv(data_dir+'mnli_train.tsv', sep=\"\\t\", index_col = False)\n",
    "mnli_val = pd.read_csv(data_dir+'mnli_val.tsv', sep=\"\\t\", index_col=False)\n",
    "mnli_genre = mnli_train.genre.unique().tolist()\n",
    "print('there are %d genres in MNLI:'%len(mnli_genre), mnli_genre)\n",
    "target_dic = {'neutral':1,'entailment':2, 'contradiction':0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_tokens = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "mnli_val_tokens = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "for genre in mnli_genre:\n",
    "    mnli_train_tokens[genre]['sent1'], _ = tokenize_dataset(mnli_train[mnli_train.genre == genre].sentence1)\n",
    "    mnli_train_tokens[genre]['sent2'], _ = tokenize_dataset(mnli_train[mnli_train.genre == genre].sentence2)\n",
    "    mnli_train_tokens[genre]['label'] = [target_dic[j] for j in mnli_train[mnli_train.genre == genre].label]\n",
    "    mnli_val_tokens[genre]['sent1'], _ = tokenize_dataset(mnli_val[mnli_val.genre == genre].sentence1)\n",
    "    mnli_val_tokens[genre]['sent2'], _ = tokenize_dataset(mnli_val[mnli_val.genre == genre].sentence2)\n",
    "    mnli_val_tokens[genre]['label'] = [target_dic[j] for j in mnli_val[mnli_val.genre == genre].label]\n",
    "    \n",
    "pkl.dump(mnli_train_tokens, open(data_dir+'mnli_train_tokens.p', 'wb'))\n",
    "pkl.dump(mnli_val_tokens, open(data_dir+'mnli_val_tokens.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_tokens = pkl.load(open(data_dir+'mnli_train_tokens.p', 'rb'))\n",
    "mnli_val_tokens = pkl.load(open(data_dir+'mnli_val_tokens.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary: 18105\n"
     ]
    }
   ],
   "source": [
    "embeddings = pkl.load(open(data_dir+'embeddings.p', 'rb'))\n",
    "\n",
    "def build_vocab(embeddings, tokens_data,  max_vocab_size = len(embeddings.keys())):\n",
    "\n",
    "    #if max_vocab_size:\n",
    "    all_tokens = [i for tokens in tokens_data for i in tokens]\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    vocab = [i for i in vocab if i in embeddings.keys() ]\n",
    "    vocab_len = len(vocab)\n",
    "    print('length of vocabulary:',vocab_len) \n",
    "\n",
    "    id2token = list(vocab)                     \n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    id2vector = np.zeros((len(id2token), 300))\n",
    "    for i, word in enumerate(id2token):\n",
    "        try:\n",
    "            id2vector[i] = embeddings[word]\n",
    "        except KeyError:\n",
    "            id2vector[i] = np.random.normal(scale = 0.1, size = (300,))\n",
    "\n",
    "    return id2token, token2id, id2vector\n",
    "\n",
    "train_sent1_tokens = pkl.load(open(data_dir+'train_sent1_tokens.p', 'rb'))\n",
    "train_sent2_tokens = pkl.load(open(data_dir+'train_sent2_tokens.p', 'rb'))\n",
    "# vocalbulary generated by SNLI Data\n",
    "id2token, token2id, id2vector = build_vocab(embeddings, train_sent1_tokens+train_sent2_tokens)\n",
    "vocab_size = len(id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_indices = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "mnli_val_indices = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "for genre in mnli_genre:\n",
    "    mnli_train_indices[genre]['sent1'] = token2index_dataset(mnli_train_tokens[genre]['sent1'], token2id)\n",
    "    mnli_train_indices[genre]['sent2'] = token2index_dataset(mnli_train_tokens[genre]['sent2'], token2id)\n",
    "    mnli_val_indices[genre]['sent1'] = token2index_dataset(mnli_val_tokens[genre]['sent1'], token2id)\n",
    "    mnli_val_indices[genre]['sent2'] = token2index_dataset(mnli_val_tokens[genre]['sent2'], token2id)\n",
    "pkl.dump(mnli_train_indices, open(data_dir+'mnli_train_indices.p', 'wb'))  \n",
    "pkl.dump(mnli_val_indices, open(data_dir+'mnli_val_indices.p', 'wb'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_indices = pkl.load(open(data_dir+'mnli_train_indices.p', 'rb'))\n",
    "mnli_val_indices = pkl.load(open(data_dir+'mnli_val_indices.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_dataset = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "mnli_val_dataset = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "mnli_train_loader = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "mnli_val_loader = {'telephone':{}, 'fiction':{},'slate':{}, 'government':{}, 'travel':{}}\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "for genre in mnli_genre:\n",
    "    mnli_train_dataset[genre] = NLI_Dataset(mnli_train_indices[genre]['sent1'], mnli_train_indices[genre]['sent2'], mnli_train_tokens[genre]['label'] )\n",
    "    mnli_train_loader[genre] = torch.utils.data.DataLoader(dataset=mnli_train_dataset[genre], \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    mnli_val_dataset[genre] = NLI_Dataset(mnli_val_indices[genre]['sent1'], mnli_val_indices[genre]['sent2'], mnli_val_tokens[genre]['label'] )\n",
    "    mnli_val_loader[genre] = torch.utils.data.DataLoader(dataset=mnli_val_dataset[genre], \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load best model\n",
    "model_RNN = pkl.load(open('./models/best_RNN.sav', 'rb'))\n",
    "model_CNN = pkl.load(open('./models/best_CNN.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent1_tokens = pkl.load(open(data_dir+'train_sent1_tokens.p', 'rb'))\n",
    "train_sent2_tokens = pkl.load(open(data_dir+'train_sent2_tokens.p', 'rb'))\n",
    "train_target = pkl.load(open(data_dir+'train_target.p', 'rb'))\n",
    "\n",
    "val_sent1_tokens = pkl.load(open(data_dir+'val_sent1_tokens.p', 'rb'))\n",
    "val_sent2_tokens = pkl.load(open(data_dir+'val_sent2_tokens.p', 'rb'))\n",
    "val_target = pkl.load(open(data_dir+'val_target.p', 'rb'))\n",
    "\n",
    "train_sent1_indices = token2index_dataset(train_sent1_tokens, token2id)\n",
    "train_sent2_indices = token2index_dataset(train_sent2_tokens, token2id)\n",
    "val_sent1_indices = token2index_dataset(val_sent1_tokens, token2id)\n",
    "val_sent2_indices  = token2index_dataset(val_sent2_tokens, token2id)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NLI_Dataset(train_sent1_indices, train_sent2_indices, train_target )\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NLI_Dataset(val_sent1_indices, val_sent2_indices,val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=NLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CNN_val_acc    RNN_val_acc\n",
      "----------  -------------  -------------\n",
      "telephone           41.00          45.37\n",
      "fiction             42.71          45.93\n",
      "slate               42.02          43.51\n",
      "government          43.60          43.31\n",
      "travel              43.18          43.28\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "val_acc_table = {'RNN_val_acc':[], 'CNN_val_acc':[] }\n",
    "\n",
    "for genre in mnli_genre:\n",
    "    val_acc_table['RNN_val_acc'].append(test_model(mnli_val_loader[genre], model_RNN)[0])\n",
    "    val_acc_table['CNN_val_acc'].append(test_model(mnli_val_loader[genre], model_CNN)[0])\n",
    "    \n",
    "val_acc_table = pd.DataFrame(val_acc_table, index = mnli_genre)\n",
    "print (tabulate(val_acc_table, floatfmt=\".2f\", headers = val_acc_table.columns))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on MultiNLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning on genre telephone:\n",
      "Fine_tuning_RNN\n",
      "number of trainable parameters:487603\n",
      "Val Accuracy:52.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre fiction:\n",
      "Fine_tuning_RNN\n",
      "number of trainable parameters:487603\n",
      "Val Accuracy:49.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre slate:\n",
      "Fine_tuning_RNN\n",
      "number of trainable parameters:487603\n",
      "Val Accuracy:44.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre government:\n",
      "Fine_tuning_RNN\n",
      "number of trainable parameters:487603\n",
      "Val Accuracy:53.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre travel:\n",
      "Fine_tuning_RNN\n",
      "number of trainable parameters:487603\n",
      "Val Accuracy:51.0%\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "RNN_fine_tune_acc = []\n",
    "for genre in mnli_genre:\n",
    "    print('Fine tuning on genre %s:'%genre)\n",
    "    _, _, _, _ = train_model(model_RNN,  mnli_train_loader[genre], mnli_val_loader[genre], 3e-3, 5, 'Fine_tuning_RNN', True, False, False)    \n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning on genre telephone:\n",
      "Fine-tuning Model\n",
      "number of trainable parameters:250603\n",
      "Val Accuracy:50.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre fiction:\n",
      "Fine-tuning Model\n",
      "number of trainable parameters:250603\n",
      "Val Accuracy:47.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre slate:\n",
      "Fine-tuning Model\n",
      "number of trainable parameters:250603\n",
      "Val Accuracy:46.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre government:\n",
      "Fine-tuning Model\n",
      "number of trainable parameters:250603\n",
      "Val Accuracy:49.7%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine tuning on genre travel:\n",
      "Fine-tuning Model\n",
      "number of trainable parameters:250603\n",
      "Val Accuracy:48.3%\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fine_tune_acc = []\n",
    "for genre in mnli_genre:\n",
    "    print('Fine tuning on genre %s:'%genre)\n",
    "    _, _, _, _ = train_model(model_CNN,  mnli_train_loader[genre], mnli_val_loader[genre], \n",
    "                                     3e-3, 5, 'Fine-tuning Model', False, True, False)\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CNN_val_acc    RNN_val_acc    RNN_val_acc_fine_tune    CNN_val_acc_fine_tune\n",
      "----------  -------------  -------------  -----------------------  -----------------------\n",
      "telephone           41.00          45.37                    52.80                    50.30\n",
      "fiction             42.71          45.93                    49.50                    47.70\n",
      "slate               42.02          43.51                    44.70                    46.30\n",
      "government          43.60          43.31                    53.80                    49.70\n",
      "travel              43.18          43.28                    51.00                    48.30\n"
     ]
    }
   ],
   "source": [
    "val_acc_table['RNN_val_acc_fine_tune'] = [52.8, 49.5, 44.7, 53.8, 51.0 ]\n",
    "val_acc_table['CNN_val_acc_fine_tune'] = [50.3, 47.7, 46.3, 49.7, 48.3 ]\n",
    "print (tabulate(val_acc_table, floatfmt=\".2f\", headers = val_acc_table.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
